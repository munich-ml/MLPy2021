{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nav_menu": {
      "height": "279px",
      "width": "309px"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "22_end2end_ml_project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BvxUc1q8O4-L",
        "fN6ao_3KkznT",
        "xHhRDXfHFv0g"
      ],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/munich-ml/MLPy2021/blob/main/22_end2end_ml_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqQTETRT-NjI"
      },
      "source": [
        "# Intro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9_a-EzZ6x8B"
      },
      "source": [
        "\n",
        "## References\n",
        "Resources used to create this notebook:\n",
        "- [scikit-learn website](https://scikit-learn.org)\n",
        "- [Matplotlib website](https://matplotlib.org/)\n",
        "- [Wikipedia](https://en.wikipedia.org/wiki/Main_Page)\n",
        "- Hands-on Machine Learning with Scikit-learn, Keras & TensorFlow, Aurelien Geron, [Book on Amazon](https://www.amazon.de/Aur%C3%A9lien-G%C3%A9ron/dp/1492032646/ref=sr_1_3?__mk_de_DE=%C3%85M%C3%85%C5%BD%C3%95%C3%91&dchild=1&keywords=Hands-on+Machine+Learning+with+Scikit-learn%2C+Keras+%26+TensorFlow%2C+Aurelien+Geron%2C&qid=1589875241&sr=8-3)\n",
        "- Introduction to Machine Learning with Python, Andreas Mueller, [Book on Amazon](https://www.amazon.de/Introduction-Machine-Learning-Python-Scientists/dp/1449369413)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci0n-UMbun5n"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5Z0FqxuMz1t"
      },
      "source": [
        "# Python â‰¥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Common imports\n",
        "import os, pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# Setup matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Ignore useless warnings (see SciPy issue #5998)\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
        "\n",
        "in_colab = 'google.colab' in sys.modules   # check if note is executed within Colab\n",
        "\n",
        "# Clone the repository if executed in Google Colab\n",
        "if in_colab:  \n",
        "    if \"MLPy2021\" in os.listdir():\n",
        "        !git -C MLPy2021 pull\n",
        "    else:\n",
        "        !git clone https://github.com/munich-ml/MLPy2021/\n",
        "\n",
        "# lib.helper_funcs.py. The import path depends on Colab or local execution \n",
        "if in_colab:\n",
        "    from MLPy2021.lib.helper_funcs import plot_hist_2D\n",
        "else: \n",
        "    from lib.helper_funcs import plot_hist_2D\n",
        "\n",
        "# The import path depends on Colab or local execution\n",
        "if in_colab:\n",
        "    base_dir = os.path.join(os.getcwd(), \"MLPy2021\")\n",
        "else:\n",
        "    base_dir = os.getcwd()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw_zMxdxun5v"
      },
      "source": [
        "# Get the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CbQ09Aeu0R3"
      },
      "source": [
        "At this point the dataset has already been cloned from GitHub and is located in the `datasets` subdirectory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sUzRkQmvJX2"
      },
      "source": [
        "data_dir = os.path.join(base_dir, \"datasets\")\n",
        "os.listdir(data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaSqQsQYfieR"
      },
      "source": [
        "with open(os.path.join(data_dir, \"housing_dataset.pkl\"), \"rb\") as file:\n",
        "    housing = pickle.load(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjz1RAXkvVNo"
      },
      "source": [
        "# A quick look at the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr9BGcL8xBM_"
      },
      "source": [
        "The **housing dataset** contains house pricing data of about 20K districts in California. The type of the dataset is a Pandas DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsqNXkUVm37z"
      },
      "source": [
        "housing.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUpqXYjwwPT2"
      },
      "source": [
        "type(housing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpD513dZyUMm"
      },
      "source": [
        "housing.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9z3h7rEun59"
      },
      "source": [
        "housing.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHKdLgswwTfY"
      },
      "source": [
        "Interpreting the `tail` and `info` printout:\n",
        "- Each row (``index`` in Pandas) represents one district (one measurement in general)\n",
        "- Each column represents one **attribute**\n",
        "- The units of the attributes are mostly arbitrary\n",
        "- The `total_bedrooms` attribute seems to be incomplete\n",
        "- All attributes are numeric (`Dtype=float64`), except for `ocean_proximity`\n",
        "- The `ocean_proximity` attribute contains text. It is no arbitrary text, but **categorical**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWUld1Qe0eEZ"
      },
      "source": [
        "set(housing[\"ocean_proximity\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxp85nGzun6A"
      },
      "source": [
        "housing[\"ocean_proximity\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwKJGSRoi096"
      },
      "source": [
        "### Exercise on Pandas\n",
        "\n",
        "Show the 5 data samples with ``\"ocean_proximity\"`` being ``\"ISLAND\"``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lkeWINXjj-Y"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbrtxPLejMsT"
      },
      "source": [
        "mask = housing[\"ocean_proximity\"] == \"ISLAND\"\n",
        "housing.loc[mask, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_Yl8ano2HWA"
      },
      "source": [
        "## Attribute histograms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs_7C9bO26EN"
      },
      "source": [
        "Pandas offers build-in histgramm plots of all columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOgs4l2yun6I"
      },
      "source": [
        "housing.hist(bins=50, figsize=(13, 6))\n",
        "plt.tight_layout();  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTbHdD_Osjka"
      },
      "source": [
        "Histogramms for **categorical features** have to be called from the **Series**: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP3xFTQ3kop_"
      },
      "source": [
        "housing[\"ocean_proximity\"].hist(figsize=(8,3));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETnGW4D83PiU"
      },
      "source": [
        "#### Observations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmMJIV6QtcLQ"
      },
      "source": [
        "cols = [\"median_income\", \"housing_median_age\", \"median_house_value\"]\n",
        "housing[cols].hist(bins=50, figsize=(13, 3), layout=(1, len(cols)));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgYKlaxz3e-g"
      },
      "source": [
        "Info given in the dataset describtion: The `median_income` unit is arbitrary. Furthermore, its values are capped at 15 (for high incomes) and 0.5 (for low incomes)\n",
        "\n",
        "\n",
        "Obviously `housing_median_age` and `median_house_value` are capped, too. Could become a problem beause:\n",
        "\n",
        "- `medial_house_value` will later be the **target attribute**, also called **label**\n",
        "- the number of capped samples is significant\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NEzCDRlDwcn"
      },
      "source": [
        "# Split data into **training** and **test** data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjzRBAGP2PT3"
      },
      "source": [
        "It is time to split the data set into a **training set** and a **test set** for final evaluation / test. \n",
        "\n",
        "![split_train_test.png](https://github.com/munich-ml/MLPy2021/blob/main/images/split_train_test.png?raw=1)\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WkxOK0zs0CZ"
      },
      "source": [
        "### Why splitting test data so early ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5eRpUjwsz9E"
      },
      "source": [
        "Humans are strong in pattern recognition.\n",
        "\n",
        "If the entire dataset is studied thoroughly, the data scientist may chose a paricular model that **overfits**.\n",
        "\n",
        "The **generalization error** would be estimated too optimistic and the system would perform worse than expected on new data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYcZI_XDtXmq"
      },
      "source": [
        "## Unterstanding `train_test_split`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dERIR13RCfD_"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDWisThMxWwZ"
      },
      "source": [
        "**`train_test_split`** features:\n",
        "- Easy to use 'one-liner'\n",
        "- **Randomly** splits data in an arbitrary size\n",
        "- Reproducable splitting, so the model doesn't see more of the full dataset on each execution\n",
        "- Multiple datasets can be split on the same indices (e.g. `features` and `labels`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3yc01ds2R1v"
      },
      "source": [
        "### Exercise `train_test_split`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc345UJe2yzi"
      },
      "source": [
        "Evaluate `train_test_split` function using some simple artificial dataset with the 200 labels being 0's followd by 100 1's."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQzROtj4HkiD"
      },
      "source": [
        "N = 300\n",
        "feature = np.arange(0, N, step=1)   # 0..N\n",
        "labels = np.zeros_like(feature)\n",
        "labels[feature > feature.max() * 2/3] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQewXicXbRIO"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdfcnSzGbay4"
      },
      "source": [
        "1. Create a DataFrame ``dataset`` with the columns ``feature``. and ``labels``\n",
        "\n",
        "1. Use ``train_test_split()`` into a ``train_set`` and a ``test_set`` of equal size. Use ``?`` for help.\n",
        "\n",
        "1. Evaluate the histogramms from all three sets. Was the split successful?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Rh5UIRkeYg0"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BADy8_sOHkft"
      },
      "source": [
        "dataset = pd.DataFrame(np.column_stack([feature, labels]), columns=[\"feature\", \"labels\"])\n",
        "dataset.iloc[[0, 1, 199, 200, -2, -1], :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmM55JA2KEeG"
      },
      "source": [
        "train_set, test_set = train_test_split(dataset, test_size=0.5, random_state=4711)\n",
        "train_set.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAy2N0rQfQT-"
      },
      "source": [
        "for set_ in (dataset, train_set, test_set):\n",
        "    set_.hist(figsize=[10, 1.5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cR11Z3K7Pkv"
      },
      "source": [
        "`test_train_split` worked as expected:\n",
        "- both subsets are ramdomly shuffled\n",
        "- each of the subsets `train_set` and `test_set` contains about 100 0's and 50 1's\n",
        "- executing `test_train_split` multiple times returns identical results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkq3_7eWtqmj"
      },
      "source": [
        "## Unterstanding `StratifiedShuffleSplit`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooDxE5EsAWZj"
      },
      "source": [
        "**Stratified sampling** ensures that the feature \n",
        "distributions are **representative**. \n",
        "\n",
        "The `test_train_split` function we used before didn't split the dataset represantatively:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYchLmrP6gqO"
      },
      "source": [
        "train_set[\"labels\"].sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B34y_PTx8eyw"
      },
      "source": [
        "test_set[\"labels\"].sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SBokH-0DunY"
      },
      "source": [
        "... `54 / 46` instead of `50 / 50` is a significant difference and is called **sampling bias**.\n",
        "\n",
        "The sampling bias effect is typically worse for smaller datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sttBzuPFCoU"
      },
      "source": [
        "Scikit-Learn incorporates the `StratifiedShuffleSplit` class, unfortunatelly without a handy one-liner function such as `test_train_split`. Thus the code is more complex:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMQso2DxHkcu"
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCvYtyy58cDj"
      },
      "source": [
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=23)\n",
        "for train_index, test_index in split.split(dataset, dataset[\"labels\"]):\n",
        "    strat_train_set = dataset.loc[train_index]\n",
        "    strat_test_set = dataset.loc[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6iR4QV3Fmhi"
      },
      "source": [
        "Checking the subsets reveals a perfectly representative `50 / 50` split: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDJ5HEBaAP38"
      },
      "source": [
        "strat_train_set[\"labels\"].sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcxEiHsU8cBA"
      },
      "source": [
        "strat_test_set[\"labels\"].sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz7w9-b38b-b"
      },
      "source": [
        "for set_ in (strat_train_set, strat_test_set):\n",
        "    set_.hist(figsize=[10, 1.5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auKaDNFHIRq7"
      },
      "source": [
        "## Applying `StratifiedShuffleSplit`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVGwctlGI1by"
      },
      "source": [
        "Applying `StratifiedShuffleSplit` to the housing data is a bit more complicated than for the simple `dataset`. It has to be decided on which feature to stratify on, an that feature needs to be *categorical*.\n",
        "\n",
        "In this example:\n",
        "- `\"median_income\"` is chosen a *stratification feature*\n",
        "- `pd.cut` is used to cut is in bins in order to have it  *categorical*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWIwDcU7v2w1"
      },
      "source": [
        "Add a new cathegorical feature `income_cat`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz3isVx1hqPi"
      },
      "source": [
        "housing[\"median_income\"].hist(bins=100, figsize=(10,3));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjCbRsGYun62"
      },
      "source": [
        "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"], \n",
        "                               bins=[0., 1.5, 2.5, 3.5, 4.5, 6., np.inf])   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDcjeNg5v9fG"
      },
      "source": [
        "Split the `housing` dataset representatively to `income_cat`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2u1NnLso0ve"
      },
      "source": [
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
        "    strat_train_set = housing.loc[train_index]\n",
        "    strat_test_set = housing.loc[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P7C-tYUjolb"
      },
      "source": [
        "strat_train_set.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3uV6QsuwTPZ"
      },
      "source": [
        "Finally, remove the `income_cat` feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSoLBS8Hm2mk"
      },
      "source": [
        "for set_ in (housing, strat_train_set, strat_test_set):\n",
        "    set_.drop(\"income_cat\", axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHy1iN8BwsKU"
      },
      "source": [
        "The histogramms suggest that the represantative split was successful:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXEtS6rKo0y3"
      },
      "source": [
        "for set_ in (housing, strat_train_set, strat_test_set):\n",
        "    set_[\"median_income\"].hist(figsize=[10, 3], bins=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wyv6AOsNTI8"
      },
      "source": [
        "The test subset needs to be kept for later use!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMUKWt8lM8Uo"
      },
      "source": [
        "strat_test_set.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUA2daGexOTV"
      },
      "source": [
        "The training subset is small enough to explore is small enough to explore is directly. Otherwise we would split a smaller *exploration subset*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5v7aMauNAFV"
      },
      "source": [
        "strat_train_set.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6clU0VENKLD"
      },
      "source": [
        "Now, we copy the training subset back to the more descriptive `housing` variable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MTfsmycun7I"
      },
      "source": [
        "housing = strat_train_set.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH7nRhqbun7H"
      },
      "source": [
        "# Explore the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94iFe_uIxmLD"
      },
      "source": [
        "housing.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iLV33F8lZeo"
      },
      "source": [
        "There is geographical information in the dataset, so let's plot the map:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOnKsmlVx4Dx"
      },
      "source": [
        "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTogLkXQx-bk"
      },
      "source": [
        "Use transparency (`alpha=0.1`) to see high-density areas better:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yqq71QT-un7P"
      },
      "source": [
        "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwxNwd8qlwtK"
      },
      "source": [
        "The following graph puts more information in the 2D graph:\n",
        "- color: `median_house_value`, the target / label\n",
        "- size: `population` of the district"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CooVv-ljmvuE"
      },
      "source": [
        "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, s=housing[\"population\"]/100, \n",
        "             figsize=(10, 7), c=\"median_house_value\", cmap=plt.cm.jet, colorbar=True, sharex=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4GLoT25un7T"
      },
      "source": [
        "Bugfix: Use `sharex=False`, otherwise the x-axis values and legend are not displayed (https://github.com/pandas-dev/pandas/issues/10611). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buouYG0w_9s_"
      },
      "source": [
        "The graph is even more self-explaining when overlayed with a map. In this example, the map is loaded as an image (`california.png`) and \"aligned-by-hand\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oJb8mSCun7Z"
      },
      "source": [
        "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, s=housing[\"population\"]/100, \n",
        "             figsize=(10, 7), c=\"median_house_value\", cmap=plt.cm.jet, colorbar=True, sharex=False)\n",
        "\n",
        "plt.imshow(mpl.image.imread(os.path.join(base_dir, \"images\", \"california.png\")), \n",
        "           extent=[-124.55, -113.80, 32.45, 42.05]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWM22PFFBBLY"
      },
      "source": [
        "## Interpreting the map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc_HNa1wBHUQ"
      },
      "source": [
        "- the `median_house_prices` are strongly related to location\n",
        "- `ocean_proximity` could be helpful. Although there are low-value coastal areas, as well.\n",
        "- a clustering algorithm should easily find high price areas and a new features with distances to these cluster centers would be useful "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sez8lE734xw"
      },
      "source": [
        "## Correlations "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2P5sIL5lUzA"
      },
      "source": [
        "Computing the correlation coefficients is easy and can provide insides to feature-to-feature and feature-to-target assiciations. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-xjktnvnPe1"
      },
      "source": [
        "### Correlation coefficient\n",
        "A **correlation coefficient $p$** shows how much two features' movements are associated *linearly*:\n",
        "\n",
        "- $abs(p) < 0.1$: No relevant linear correlation\n",
        "- $p > 0.5$: Strong plositive linear correlation\n",
        "- $p < -0.5$: Strong negative linear correlation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qf-foW4kogV"
      },
      "source": [
        "#### Correlation of arbitrary functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbWNfFDMkxoZ"
      },
      "source": [
        "Let's simulate some arbitraty funcitons to get a feeling for the correlation coefficients:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBYGaUSBQWBh"
      },
      "source": [
        "x = np.linspace(start=-1, stop=0.85, num=100)\n",
        "np.random.seed(42)\n",
        "noise = 0.4 * np.random.randn(x.size)\n",
        "df = pd.DataFrame(np.column_stack([ x , 0.33*x,   x*x    ,  -x  ,  noise ,  x + noise]), \n",
        "                          columns=[\"x\", \"pos\" ,  \"square\", \"neg\", \"noise\", \"pos+noise\"])\n",
        "df.plot(style=\".\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OnaDAd8diZH"
      },
      "source": [
        "Panda DataFrames include a `corr` method for computing the pairwise correlation coefficients of the columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa-awszqdkS8"
      },
      "source": [
        "df.corr()[\"x\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc9kHeqbmE9j"
      },
      "source": [
        "Result discussion:\n",
        "- Full correlation (+/-1) is given, regardless of the slope: E.g. $p(0.33*x)=p(x)=1$\n",
        "- The `square` function exebits a relatively weak **linear** correlation (Strongly depencent on the x-axis section)\n",
        "- Although quite noisy the `pos+noise` correlation is very strong.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skezkbvJn_b6"
      },
      "source": [
        "### Applying `corr` to the `housing` dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwYES-9Ds5Ho"
      },
      "source": [
        "housing.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHepKaUFun7b"
      },
      "source": [
        "corr_matrix = housing.corr()\n",
        "corr_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUZJuy3M6Tw9"
      },
      "source": [
        "A heatmap of the correlation matrix is easier to evaluate. \n",
        "\n",
        "`Matplotlib` can show bitmaps by using the `imshow()` method. If the bitmap is a NumPy matrix you may call it a heatmap: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faM5KVKw7R79"
      },
      "source": [
        "plt.imshow(corr_matrix, cmap=plt.cm.bwr)\n",
        "plt.xticks(range(len(corr_matrix.columns)), labels=corr_matrix.columns, rotation=90)\n",
        "plt.yticks(range(len(corr_matrix.index)), labels=corr_matrix.index)\n",
        "plt.clim([-1, 1])\n",
        "plt.colorbar();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_NNhNm7gGID"
      },
      "source": [
        "#### Evaluating the heatmap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NStUKG4f9ia1"
      },
      "source": [
        "1. There is a strong correlation between the four features `total_rooms`, `total_bedrooms`, `population` , `households`.\n",
        "\n",
        "1. Also there is a strong negative correlation between `longitude` and `latitude`. This can be understood from the map of California from the previous chapter.\n",
        "\n",
        "1. `median_house_value` is the **target**. Thus, features with high (positive or negative) correlation with `median_house_value` are of interest. The following sorted output shows that only `median_income` correlates significantly:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRRrlwh5gWgg"
      },
      "source": [
        "#### Correlation with \"median_house_value\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4QXzJCBgcmN"
      },
      "source": [
        "The important correlation coefficients are those referencing to `median_house_value`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk2gC5h4un7f"
      },
      "source": [
        "corr_with_house_value = corr_matrix[\"median_house_value\"].drop(\"median_house_value\")\n",
        "corr_with_house_value.sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ9z4u3ylLQ6"
      },
      "source": [
        "corr_with_house_value.sort_values().plot.barh(grid=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvxUc1q8O4-L"
      },
      "source": [
        "#### Interactive correlation matrix (optional)\n",
        "For the usec-case of inspecting the four high-correlation-features in more detail, this subchapter adds interactive sliders to the color scale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwN3CjSMHsCu"
      },
      "source": [
        "from ipywidgets import interact\n",
        "from ipykernel.pylab.backend_inline import flush_figures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF9nOZ04HuUM"
      },
      "source": [
        "def plot_corr_matrix(color_min=-1, color_max=1):\n",
        "    plt.imshow(corr_matrix.values, cmap=\"bwr\")\n",
        "    ticks = corr_matrix.columns\n",
        "    plt.xticks(ticks=range(len(ticks)), labels=ticks, rotation=90)\n",
        "    plt.yticks(ticks=range(len(ticks)), labels=ticks)\n",
        "    plt.clim([color_min, color_max])\n",
        "    plt.colorbar()\n",
        "    flush_figures();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUzf1SxeHuRs"
      },
      "source": [
        "interact(plot_corr_matrix, color_min=(-1, 1, 0.01), color_max=(-1, 1, 0.01));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THZxc8s14E6X"
      },
      "source": [
        "## Scatter matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzG1xqxVEYRT"
      },
      "source": [
        "Pandas build-in `scatter_matrix` method allows quick visual evaluation of feature distributions and correlations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5mRgF2nDR4c"
      },
      "source": [
        "pd.plotting.scatter_matrix(housing, figsize=(15, 10), alpha=0.25, hist_kwds={\"bins\":25});"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l_N8vOY4LiA"
      },
      "source": [
        "...one can clearly see the high correlation within the \"4x4 center\".\n",
        "\n",
        "Let's look at a subset of the attributes more closely:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm2Y91lE23QN"
      },
      "source": [
        "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
        "              \"housing_median_age\"]\n",
        "pd.plotting.scatter_matrix(housing[attributes], figsize=(12, 8), alpha=0.25, hist_kwds={\"bins\":50});"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T26v6_4SNp--"
      },
      "source": [
        "Detailed evaluation of the __target__ `median_house_value` with the best-correlating __feature__ `median_income`:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cILtg0tfun7o"
      },
      "source": [
        "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
        "             alpha=0.2, figsize=(9, 6), s=20);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wjHSh1H7x8Y"
      },
      "source": [
        "###plot observations\n",
        "What can be learned from the scatter-plot? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei-cBjYL5DX-"
      },
      "source": [
        "- there is a strong correlation\n",
        "- there seems to be a cap at y=500k (horizontal line)\n",
        "- there are more horizontal lines (e.g. at 350k) which are probably artificial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZybmRmHWXeY"
      },
      "source": [
        "### 2D histogram and contour plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIOfxcscWfD5"
      },
      "source": [
        "Plotting alternatives with `matplotlib`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM_9qpzdy1n1"
      },
      "source": [
        "plot_hist_2D(housing, \"median_income\", \"median_house_value\", bins=25, figsize=[12, 5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQJAhxuL4-Cn"
      },
      "source": [
        "## Experimenting with feature combinations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7IXIQsXeLzT"
      },
      "source": [
        "Sometimes feature combinations are more useful in predicting the target, as the features by themself. \n",
        "\n",
        "An examples: `total_rooms` is the total number of rooms in a district. Dividing `total_rooms` by `households` (the number of households in the district) yields `rooms_per_household` and is probably more helpful in predicting the value of the house.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urTMk4fed4Nn"
      },
      "source": [
        "Before adding new features, let's make a copy of the `housing` dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBU16VlRbMf9"
      },
      "source": [
        "housing_copy = housing.copy(deep=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIquXYSVf-Zg"
      },
      "source": [
        "This, again, are the correlation coefficients of the native features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtz77rUTW9Gc"
      },
      "source": [
        "corr_matrix = housing_copy.corr()\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXLx6a2qgPTk"
      },
      "source": [
        "Adding new features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jWONhRfun7q"
      },
      "source": [
        "housing[\"rooms_per_household\"] = housing[\"total_rooms\"] / housing[\"households\"]\n",
        "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\n",
        "housing[\"population_per_household\"] = housing[\"population\"] / housing[\"households\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUOAdON7gW7K"
      },
      "source": [
        "... Note how readable the calculations are, using Pandas DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHfph0Zlgn6G"
      },
      "source": [
        "Correlations coefficients including the new features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKuTKg3dun7t"
      },
      "source": [
        "corr_matrix = housing.corr()\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlLchldHlEYG"
      },
      "source": [
        "Alternative (manual) printout that marks new features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcMA3o44XGFT"
      },
      "source": [
        "new_features = set(housing.columns).difference(set(housing_copy.columns))\n",
        "corr_dict = corr_matrix[\"median_house_value\"].sort_values(ascending=False).to_dict()\n",
        "for key, val in corr_dict.items():\n",
        "    s = f\"{key:30s}{val:.3f}\"\n",
        "    if key in new_features:\n",
        "        s += \"  <-- new feature --\"\n",
        "    print(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pocbCsYFXOqu"
      },
      "source": [
        "###  New features scatter matrix (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyLh3wAbofBH"
      },
      "source": [
        "features = [\"median_house_value\"] + list(new_features)\n",
        "pd.plotting.scatter_matrix(housing[features], figsize=(13, 8), alpha=0.5, hist_kwds={\"bins\":50});"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmObmaqsun71"
      },
      "source": [
        "# Prepare the data for Machine Learning algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kku2gbm-y4Fp"
      },
      "source": [
        "Seperate **features** and **labels** in the training set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoZzMiLnun73"
      },
      "source": [
        "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
        "housing_labels = strat_train_set[\"median_house_value\"].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmWrUNcP_YC5"
      },
      "source": [
        "housing.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McVE_5-j_bXW"
      },
      "source": [
        "housing_labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNp4XOSgAivw"
      },
      "source": [
        "##Data cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkHjuzg5Aqha"
      },
      "source": [
        "###Missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnZRHmUeA2XP"
      },
      "source": [
        "Most Machine Learning algorithms cannot work with missing valus. \n",
        "\n",
        "As seen before, the `total_bedrooms` feature contains missing values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VKUJjb1Arpu"
      },
      "source": [
        "housing.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9YEmJ_3Bz-C"
      },
      "source": [
        "**Find incomplete rows using `pd.DataFrame.isnull()` method**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM0UdvXbBIVW"
      },
      "source": [
        "sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\n",
        "sample_incomplete_rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmE2VtdFCy2d"
      },
      "source": [
        "#### Option 1: Drop rows "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DciQJTjhHXHZ"
      },
      "source": [
        "Drop incomplete rows using `dropna()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7YWkZSqun78"
      },
      "source": [
        "sample_incomplete_rows.dropna(subset=[\"total_bedrooms\"])    # option 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoL7Jpu8DnwJ"
      },
      "source": [
        "#### Option 2: Drop features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYyWPWXeEDRd"
      },
      "source": [
        "Drop incomplete features using `drop()`\n",
        "\n",
        "Default axis is `0` or `index`, meaning rows.\n",
        "\n",
        "Therefore chose `axis=1` or `column`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCn5op4Jun7_"
      },
      "source": [
        "sample_incomplete_rows.drop(\"total_bedrooms\", axis=1)       # option 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Dh4eXFlF7Wz"
      },
      "source": [
        "#### Option 3: `df.fillna()` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubei62ouHvTg"
      },
      "source": [
        "Fill missing values using `fillna()` \n",
        "Assume the missing value to be zero, the mean, the median, etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt6bRK8iun8B"
      },
      "source": [
        "median = housing[\"total_bedrooms\"].median()\n",
        "sample_incomplete_rows[\"total_bedrooms\"].fillna(median, inplace=True) # option 3\n",
        "sample_incomplete_rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH6xAeXRG7l9"
      },
      "source": [
        "As can be seen, imputing the median worked fine, BUT\n",
        "- the median needs to be saved to fill-in missing values during prediction!\n",
        "- during prediction, other features may contain missing values.\n",
        "\n",
        "Thus, it's recommended to use a **imputer** class, such as Scikit-learn's `SimpleImputer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjSXMKZjILTN"
      },
      "source": [
        "#### Option 4: `SimpleImputer`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCXZGRROH4h8"
      },
      "source": [
        "Fill missing values with Scikit-learn `SimpleImputer`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJqHjzojun8E"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy=\"median\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QufVjyGaun8G"
      },
      "source": [
        "Remove the text attribute because median can only be calculated on numerical attributes:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwX0aZ23un8G"
      },
      "source": [
        "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
        "housing_num.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUu1E5_Nun8I"
      },
      "source": [
        "imputer.fit(housing_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqzhMpWoJpHv"
      },
      "source": [
        "... upon `imputer.fit()` the median of each attribute has been computed and stored in the `statistics_` variable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Stjb4fzKun8K"
      },
      "source": [
        "imputer.statistics_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CW6oJ6MH036"
      },
      "source": [
        "The result should be identical to `df.median()` ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez8x4mw6H2_3"
      },
      "source": [
        "housing_num.median()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTOsjk9zun8O"
      },
      "source": [
        "The missing values are replaced using `transform` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncBX1ejJun8Q"
      },
      "source": [
        "X = imputer.transform(housing_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSDvvAkjLF6z"
      },
      "source": [
        "The result is a plain NumPy array and needs to be converted back to a pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8KtgCDOun8T"
      },
      "source": [
        "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing.index)\n",
        "housing_tr.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syB5zjAeLtAA"
      },
      "source": [
        "Let's check the result on the `sample_incomplete_rows` subset:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OTbn9Qqun8V"
      },
      "source": [
        "housing_tr.loc[sample_incomplete_rows.index, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Vn0dGVMurR"
      },
      "source": [
        "###Handling Text and Categorical Attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N225_exIun8i"
      },
      "source": [
        "Most Machine Learning algorithms requires numerical data.\n",
        "\n",
        "The *housing dataset* contains one text feature `ocean_proximity`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C82BESqUun8j"
      },
      "source": [
        "housing_cat = housing[(\"ocean_proximity\")]\n",
        "housing_cat.head(8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8hbH2ethhPP"
      },
      "source": [
        "The `ocean_proximity` feature is not arbitrary text. There are a limited number of possible values, each representing a *category*.\n",
        "\n",
        "Use `set()` or the `Pandas.Series.unique` method to get the *unique* values of a collection: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXxtiUhGXfnO"
      },
      "source": [
        "set(housing[\"ocean_proximity\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiAx8-Z6JdJg"
      },
      "source": [
        "housing[\"ocean_proximity\"].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN6ao_3KkznT"
      },
      "source": [
        "#### Ordinal encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OgL-cncIQ12"
      },
      "source": [
        "The `OrdinalEncoder` replaces the category text by numbers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCnVWE8Gun8l"
      },
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "ordinal_enc = OrdinalEncoder()\n",
        "housing_cat_encoded = ordinal_enc.fit_transform(housing_cat.values.reshape(-1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mcaxn1JMbQW"
      },
      "source": [
        "pd.DataFrame(housing_cat_encoded)[0].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXecN3f0MIA_"
      },
      "source": [
        "housing_cat.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQbP803Uk6dm"
      },
      "source": [
        "Is this what we want? Probably not, because the Machine Learning algorithm that two nearby values are somehow similar. This might be desired in cases of sorted categories, but it usually isn't. In those cases, a `OneHotEncoder` can be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COtHebnymvtT"
      },
      "source": [
        "####OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMMvj0CIun8r"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "cat_encoder = OneHotEncoder()\n",
        "housing_cat_1hot = cat_encoder.fit_transform(housing_cat.values.reshape(-1, 1))\n",
        "housing_cat_1hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVGx-__zun8s"
      },
      "source": [
        "For memory reasons, the `OneHotEncoder` class returns a **sparse scipy array** instead of a **dense numpy array**.\n",
        "\n",
        "Conversion to a dense array is provided by the `toarray()` method:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJnhywwcun8s"
      },
      "source": [
        "housing_cat_1hot.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7qfXENVun8w"
      },
      "source": [
        "cat_encoder.categories_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpwI1UWzVOIH"
      },
      "source": [
        "NumPy arrays can be ploted as image using Matplotlib and `imshow`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5xXJa9_N7yt"
      },
      "source": [
        "plt.figure(figsize=(14, 2))\n",
        "plt.imshow(housing_cat_1hot.toarray().T, interpolation='nearest', aspect='auto', cmap=plt.cm.cool)\n",
        "cats = list(cat_encoder.categories_[0])\n",
        "plt.yticks(range(len(cats)), labels=cats);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1IJP0Ouo9yB"
      },
      "source": [
        "## Custom transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6hsR-YN-70V"
      },
      "source": [
        "Further up this script we found **feature combinations** to be potentially helpful for predicting the target. \n",
        "\n",
        "Such **custom transformation functionality** can seamlessly work with **Scikit-Learn pipelines** by creating a **custom transformer class**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQMOt0NVRf9M"
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class CombinedAttrAdder(BaseEstimator, TransformerMixin):\n",
        "    # column index\n",
        "    INDEXES = {\"rooms\": 3, \"bedrooms\": 4, \"population\": 5, \"households\": 6}     \n",
        "\n",
        "    def __init__(self):  # no *args or **kargs\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self  # nothing else to do\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        i = CombinedAttrAdder.INDEXES\n",
        "        rooms_per_household = X[:, i[\"rooms\"]] / X[:, i[\"households\"]]\n",
        "        population_per_household = X[:, i[\"population\"]] / X[:, i[\"households\"]]\n",
        "        bedrooms_per_room = X[:, i[\"bedrooms\"]] / X[:, i[\"rooms\"]]\n",
        "        \n",
        "        return np.column_stack((X, rooms_per_household, population_per_household, \n",
        "                                bedrooms_per_room))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKe4wAslj96T"
      },
      "source": [
        "Instanciating the new Transformer class ...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64DyddsNisOX"
      },
      "source": [
        "attr_adder = CombinedAttrAdder()\n",
        "housing_extra_attr = attr_adder.fit_transform(housing.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzbeSJ3JlOEf"
      },
      "source": [
        "Scikit-Learn transformers receive and return Numpy arrays. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Oreq9VeMV_T"
      },
      "source": [
        "pd.DataFrame(housing_extra_attr).head()  # Conversion to DataFrame only for pretty plotting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnC0470TV40I"
      },
      "source": [
        "You have to keep track of the columns names, manually. This can easily get messy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNJFLt7CZ4X6"
      },
      "source": [
        "housing_columns = list(housing.columns) + [\"rooms_per_household\", \"population_per_household\", \n",
        "                                           \"bedrooms_per_room\"]\n",
        "housing_columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_crBd2oun8z"
      },
      "source": [
        "housing_extra_attr = pd.DataFrame(housing_extra_attr, columns=housing_columns, index=housing.index)\n",
        "housing_extra_attr.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHnmXHq5uN9i"
      },
      "source": [
        "... the resulting `housing_extra_attr` DataFrame contains the original as well as the extra attributes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULiyGfKjBeyk"
      },
      "source": [
        "## Feature scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAOOgVvHWcYr"
      },
      "source": [
        "Most Machine Mearning algorithms require some kind of **features scaling**. With features of very different scales, they don't predict well or even don't converge at all. \n",
        "\n",
        "Some algorithms even require all features to be in the range of 0...1.\n",
        "\n",
        "A notable exception are **decision tree algorithms** that are robust to arbitrarily scaled data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq90P5gNW4tT"
      },
      "source": [
        "housing.hist(bins=50, figsize=(13, 5))\n",
        "plt.tight_layout();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49XCiVitW5TC"
      },
      "source": [
        "... yes, the scales of our `housing` dataset features are very different. Thus, scaling those features is requried.\n",
        "\n",
        "Scikit-Learn provides various scalers, while the two common ones are `MinMaxScaler` and `StandardScaler`:  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4QA2RS0uN7p"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq18ugfQ_z5q"
      },
      "source": [
        "Creating `plot_scaled_feature_hist`, a convinience function to evaluate the scalers' results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQVMu_q0N-7C"
      },
      "source": [
        "def plot_scaled_feature_hist(scaler=None):\n",
        "    df = housing.drop(\"ocean_proximity\", axis=1)  # dropping the non-numeric feature\n",
        "    if scaler is not None:\n",
        "        df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
        "    df.hist(bins=50, figsize=(13, 4))\n",
        "    plt.tight_layout();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5cXc8BsDEMn"
      },
      "source": [
        "The scaler is fitted only to the training data, excluding the test data! \n",
        "\n",
        "After `scalter.fit(training_data)`, the scalining parameters (`min, max, scale` in case of `MinMaxScaler`) are stored within the scaler object.\n",
        "\n",
        "Those parameters can later be reused for transformations of test data or new data (in production). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aC_eQVnEp9L"
      },
      "source": [
        "For comparison to the scaled features, let's plot the original histograms ones again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjiltDDbRZpv"
      },
      "source": [
        "plot_scaled_feature_hist(scaler=None)  # original data without scaling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vfgYwGwRZwB"
      },
      "source": [
        "plot_scaled_feature_hist(scaler=MinMaxScaler())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft0-0Sv1Afdu"
      },
      "source": [
        "... all features are scaled between 0 and 1.\n",
        "\n",
        "Let's compare to *standardization*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS4ahGwUVdnG"
      },
      "source": [
        "plot_scaled_feature_hist(scaler=StandardScaler())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLxWYk90FBfC"
      },
      "source": [
        "... The results of `MinMaxScaler` and `StandardScaler` are very similar, just scale and offset are different.\n",
        "\n",
        "These *linear scalers* preserve the original distribution. Sometimes odd distributions, such as the `total_bedrooms` disctibution shoud be avoided. The `QuantileTransformer` shapes the output distribution gaussion / `normal` or `uniform`:  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdpZ5kGeVdlc"
      },
      "source": [
        "plot_scaled_feature_hist(scaler=QuantileTransformer(output_distribution=\"normal\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkWQqQkrVdhh"
      },
      "source": [
        "plot_scaled_feature_hist(scaler=QuantileTransformer(output_distribution=\"uniform\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9wkLggsvAii"
      },
      "source": [
        "# Transformation pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKSClg7Lun81"
      },
      "source": [
        "`Pipeline` is a Scikit-Learn class that executes data transformation steps in a defined order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7RZOzKuun81"
      },
      "source": [
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FMAIZ_ELPul"
      },
      "source": [
        "The Pipeline constructor expects a list of name, transformer tuples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fQi44Wkujrf"
      },
      "source": [
        "num_pipe = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),\n",
        "                     ('attr_adder', CombinedAttrAdder()),\n",
        "                     ('std_scaler', StandardScaler())])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg-aheF2KCwi"
      },
      "source": [
        "Having generated the `num_pipi` pipeline, allows calling `fit()` and `fit_transform` on the pipeline object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdnggfBpIEkW"
      },
      "source": [
        "housing_num_tr = num_pipe.fit_transform(housing_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq4QRASgrpb4"
      },
      "source": [
        "pd.DataFrame(housing_num_tr).hist(figsize=(13, 4), bins=100);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERCJfNx8MAIm"
      },
      "source": [
        "## Pipeline with categorical features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUGXl2tJMzqW"
      },
      "source": [
        "It would be even more convenient to have one combined pipeline for all features, numerical and categorical ones. \n",
        "\n",
        "Scikit-Learn's `ColumnTransformer` does exactly this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-buJhh6aL6H9"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_wBK_xXN6gc"
      },
      "source": [
        "num_attr = list(housing_num.columns)\n",
        "cat_attr = [\"ocean_proximity\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iKB1oE3un85"
      },
      "source": [
        "full_pipe = ColumnTransformer([(\"num\", num_pipe, num_attr),\n",
        "                               (\"cat\", OneHotEncoder(), cat_attr)])\n",
        "\n",
        "housing_prep = full_pipe.fit_transform(housing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dboGQYwT48ei"
      },
      "source": [
        "full_pipe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd4ePb6cQEyV"
      },
      "source": [
        "housing.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j308oEh3QKLR"
      },
      "source": [
        "### Quiz\n",
        "`housing` has 9 columns. How many does `housing_prep` have? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41moPA2jun8-"
      },
      "source": [
        "housing_prep.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5DoLD3qRJIQ"
      },
      "source": [
        "- 4 additional columns because of the `OneHotEncoder`: $number\\_of\\_categories - 1$\n",
        "- 3 additional columns because of the `CombinedAttrAdder`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rZSde7pcF5q"
      },
      "source": [
        "Again, we have to keep track of the column names manually:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TCd2nXIZfQd"
      },
      "source": [
        "pd.Series(housing_columns)   # pd.Series() used for pretty output, only"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcU5d8P-cdxc"
      },
      "source": [
        "The extra attributes are in, but no yet the categorical attribus ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbyDVXt4bNLG"
      },
      "source": [
        "cat_encoder = full_pipe.named_transformers_[\"cat\"]\n",
        "cat_columns = list(cat_encoder.categories_[0])\n",
        "cat_columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icMsEAATUC25"
      },
      "source": [
        "housing_prep_columns = housing_columns + cat_columns\n",
        "housing_prep_columns.remove(\"ocean_proximity\")\n",
        "pd.Series(housing_prep_columns)   # pd.Series() used for pretty output, only"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rr92z2Eun9O"
      },
      "source": [
        "# Select and train a model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SmyVQDBVeIX"
      },
      "source": [
        "## Linear regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lo3mc9AMun9P"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(housing_prep, housing_labels);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZjuYfsMV86K"
      },
      "source": [
        "... done! We got a simple but working Linear Regression. Let's try it on some instances from the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXGa1YwcXuc5"
      },
      "source": [
        "n = 5\n",
        "some_labels = strat_test_set[\"median_house_value\"].iloc[:n]\n",
        "some_data = strat_test_set.drop(\"median_house_value\", axis=1).iloc[:n]\n",
        "some_data_prep = full_pipe.transform(some_data)\n",
        "some_predictions = lin_reg.predict(some_data_prep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjSK2ZoyaEwm"
      },
      "source": [
        "Note the beauty of the transformation pipeline. All data preparations are executed by simply calling `full_pipe.transform()`.\n",
        "\n",
        "Now let's see how well it predicts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUPMIDL5YINt"
      },
      "source": [
        "a_trial = pd.DataFrame(np.column_stack((some_labels.values, some_predictions)), \n",
        "                       columns=[\"labels\", \"predictions\"])\n",
        "a_trial[\"error\"] = a_trial[\"predictions\"] - a_trial[\"labels\"]\n",
        "a_trial[\"error %\"] = a_trial[\"error\"] / a_trial[\"labels\"] * 100\n",
        "a_trial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGZIadb0YILH"
      },
      "source": [
        "a_trial.plot(\"labels\", \"predictions\", kind=\"scatter\", grid=True, figsize=(5, 5))\n",
        "for lim_func in (plt.xlim, plt.ylim):\n",
        "    lim_func((0, max(housing_labels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ELODaNbbTQ1"
      },
      "source": [
        "Not bad! There is at least basic functionality of the simple model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJlpeQpycEyJ"
      },
      "source": [
        "## Root mean squared error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYS8BO30cSAs"
      },
      "source": [
        "Scikit-Learn provides the `mean_squared_error` as a function, an NumPy will compute the square root:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaKnqsTdTLlO"
      },
      "source": [
        "$$\\text{MSE}(y_\\text{true}, y_\\text{pred}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (y_\\text{pred} - y_\\text{true})^2$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjCvxnlJdox-"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    print(\"RMSE={:.1f}\".format(rmse))\n",
        "    return rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ujiBX6Rc31m"
      },
      "source": [
        "Let's measure the RMSE on the entire training set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy0zB19mun9a"
      },
      "source": [
        "housing_predictions = lin_reg.predict(housing_prep)\n",
        "lin_rmse = root_mean_squared_error(housing_labels, housing_predictions);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2LrRFZph9-O"
      },
      "source": [
        "... is that number ok?\n",
        "\n",
        "The RSME is the typical error of the prediction. It is an absolute accuracy measure, so we have to compare it's size to the labels size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uE723x-dU31"
      },
      "source": [
        "housing_labels.hist(bins=30, figsize=(10,2), label=\"labels histogram\")\n",
        "plt.plot([0, lin_rmse], [500, 500], linewidth=3, label=\"RMSE\")\n",
        "plt.xlabel(\"median house value\"), plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkyTjsE7lO8q"
      },
      "source": [
        "The RSME looks quite significant, when plotting it on top of the labels histogram. \n",
        "\n",
        "Thus, let's try a different model..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfoFYSAEleRt"
      },
      "source": [
        "## Decisition tree model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEhgeXQshQOv"
      },
      "source": [
        "A decision tree is a powerful machine learning model, capable of finding complex nonlinear relationships in the data. But for now, let's just use it and see how it performs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6H-5Mj8glr9"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC3dPj0tun9g"
      },
      "source": [
        "tree_reg = DecisionTreeRegressor(random_state=42)\n",
        "tree_reg.fit(housing_prep, housing_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLGLSgjxun9i"
      },
      "source": [
        "tree_rmse = root_mean_squared_error(housing_labels, tree_reg.predict(housing_prep));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQnaznqfiL8t"
      },
      "source": [
        "No error at all! \n",
        "\n",
        "We found the perfect model! ... Most likely not.\n",
        "\n",
        "It is far more likely, that the model has badly overfit the data. In other words, the model just memorized the data without much generalization capability.\n",
        "\n",
        "How can we be sure? \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCUUnJ5xun9m"
      },
      "source": [
        "## Model validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KczJK1RX4uQW"
      },
      "source": [
        "How to **validate** model (e.g. *linear regression model* `lin_reg` and *decision tree model* `tree_reg`) without touching the **test set**?\n",
        "\n",
        "Remember, the test set should be preserved for a final test right before going to production.\n",
        "\n",
        "One common solution is to split the entire data set into 3 subsets:\n",
        "\n",
        "![split_train_valid_test.png](https://github.com/munich-ml/MLPy2021/blob/main/images/split_train_valid_test.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIkRKUUu18OS"
      },
      "source": [
        "### Cross validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhgUu3w66lOl"
      },
      "source": [
        "The k-fold **cross validation** trains and validates a model k-times, each time with a differnet validation subset (called 'fold'). \n",
        "\n",
        "![images/split_cross_val.png](https://github.com/munich-ml/MLPy2021/blob/main/images/split_cross_val.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PU74Yh58E_5"
      },
      "source": [
        "- **advantage**: Cross validation provides not only the performance measure, but also an estimate of how precise that measure is (e.g. as a standard deviation).\n",
        "\n",
        "- **disadvantage**: Cross validation executes `k`-times, so it may take a long time to finish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxMFMY8XCUmm"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKZtMldP0fIm"
      },
      "source": [
        "# cross_val_score wrapper\n",
        "# conveniently returning RSME (root mean squared error) as mean and standard\n",
        "# deviation of the folds\n",
        "def cross_validation(*args, **kwargs):\n",
        "    scores = cross_val_score(*args, **kwargs,\n",
        "                             scoring=\"neg_mean_squared_error\")\n",
        "    rmse = np.sqrt(-scores)\n",
        "    print(\"Cross val mean(RMSE)={:.1f}, std(RMSE)={:.1f}\".format(rmse.mean(), \n",
        "                                                                    rmse.std()))\n",
        "    return {\"mean\":rmse.mean(), \"std\":rmse.std()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5n-GAXSAl3I"
      },
      "source": [
        "tree_val_rsme = cross_validation(tree_reg, housing_prep, housing_labels, cv=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1f8SfJfFktH"
      },
      "source": [
        "Now, the *decision tree model* doesn't look as good anymore. \n",
        "\n",
        "- test / **in-sample** RSME is 0\n",
        "- validation / **out-of-sample** RSME is large\n",
        "- that model is clearly **overfitting** the data\n",
        "\n",
        "It is even worse that the simple *linear regression model*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJssO7OfE8np"
      },
      "source": [
        "lin_val_rmse = cross_validation(lin_reg, housing_prep, housing_labels, cv=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E66u0BfqIY5-"
      },
      "source": [
        "### Cross val Exercise \n",
        "Try to **regularize** the decision tree using its `min_samples_leaf` hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t0x4AyW2iaF"
      },
      "source": [
        "min_samples_leaf = [250, 100, 50, 30, 20, 10, 6, 3, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfFsGnOm2lmf"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z_wqIRREK1E"
      },
      "source": [
        "tree_val_rsme_means = []\n",
        "for msl in min_samples_leaf:\n",
        "    tree_reg = DecisionTreeRegressor(random_state=42, min_samples_leaf=msl)\n",
        "    tree_val_rsme = cross_validation(tree_reg, housing_prep, housing_labels, cv=5)\n",
        "    tree_val_rsme_means.append(tree_val_rsme[\"mean\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmBUDtQE2qO"
      },
      "source": [
        "plt.semilogx(min_samples_leaf, tree_val_rsme_means, \"o\")\n",
        "plt.xlabel(\"min samples per leaf\"), plt.ylabel(\"tree validation RSME\")\n",
        "plt.grid(), plt.tight_layout();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl5JVung80Wj"
      },
      "source": [
        "This means the **DecisionTree** model can be improved significantly by **regulization**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVy3k4WTRrbC"
      },
      "source": [
        "## Random forest regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHn9_IiURrbL"
      },
      "source": [
        "[Random forests on scikit-learn.org](https://scikit-learn.org/stable/modules/ensemble.html#forest)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMlJZNCCdheE"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmBdR7xuun9r"
      },
      "source": [
        "forest_reg = RandomForestRegressor(random_state=42)\n",
        "forest_reg.fit(housing_prep, housing_labels)\n",
        "forest_rmse = root_mean_squared_error(housing_labels, forest_reg.predict(housing_prep));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v86wcaFHnZ8"
      },
      "source": [
        "forest_val_rmse = cross_validation(forest_reg, housing_prep, housing_labels, cv=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPTOjiosQ6tf"
      },
      "source": [
        "## Support vector machine model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFQYM3VXQ-Zi"
      },
      "source": [
        "[SVM on scikit-learn.org](https://scikit-learn.org/stable/modules/svm.html#svm)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtZ5tZLFRVS9"
      },
      "source": [
        "from sklearn.svm import SVR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8j58PCoun9y"
      },
      "source": [
        "svm_reg = SVR(kernel=\"linear\")\n",
        "svm_reg.fit(housing_prep, housing_labels)\n",
        "svm_rmse = root_mean_squared_error(housing_labels, svm_reg.predict(housing_prep))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGiSDrmXQHil"
      },
      "source": [
        "svm_val_rmse = cross_validation(svm_reg, housing_prep, housing_labels, cv=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXw15KoqT1Rf"
      },
      "source": [
        "## Comparing the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qey4zmIuIe_E"
      },
      "source": [
        "pd.DataFrame([[lin_rmse, lin_val_rmse[\"mean\"]], \n",
        "              [tree_rmse, tree_val_rsme[\"mean\"]],\n",
        "              [forest_rmse, forest_val_rmse[\"mean\"]],\n",
        "              [svm_rmse, svm_val_rmse[\"mean\"]]],\n",
        "             columns = [\"in-sample RSME\", \"out-of-sample RSME\"],\n",
        "             index = [\"linear regression\", \"decision tree\", \"random forest\", \"linear SVM\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWG4HH8fVVsQ"
      },
      "source": [
        "For now, the **random forest** is the most promising model. The fact that it's still overfitting shows that it can be further improved by **contraining**, or training to more data (which is often not possible).\n",
        "\n",
        "For the **decision tree** it has already been shown, that it can be improved by **regulization**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvE3WwmPWbTN"
      },
      "source": [
        "# Tune the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOZrw6pX5cLf"
      },
      "source": [
        "**Tuning** the model means optimizing a **score** by varying the **hyperparameters**. In the *housing example* it's the RMSE that should be optimized / minimized.  \n",
        "\n",
        "Tuning can be done manually or by using automized tools such as `GridSearchCV` or `RandomizedSearchCV`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYBV7gVC6xJY"
      },
      "source": [
        "## GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svqzngdi61RO"
      },
      "source": [
        "The Scikit-Learn `GridSearchCV` automizes the tuning process:\n",
        "- receive an `estimator` / a model to be tuned\n",
        "- *fit* data to the model and *score* the predictions (the scorer ist set by `scoring`)\n",
        "- execute the procedure for each combination of the `param_grid`\n",
        "- *cross validation* is executed on each step with `cv` defining the number of *folds*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EurwrM67Wa2k"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU8z14zOB1q6"
      },
      "source": [
        "Select an *estimator* / model and the respective *parameter grid* to be tuned:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js39O8i4CJ0Q"
      },
      "source": [
        "forest_reg = RandomForestRegressor(random_state=42, bootstrap=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRVYKdUVCN7C"
      },
      "source": [
        "param_grid = {'n_estimators': [3, 10, 30], \n",
        "              'max_features': [2, 5, 8],\n",
        "              'min_samples_leaf': [1, 3, 5]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gecczA3cAOcC"
      },
      "source": [
        "Note on **execution time**:\n",
        "\n",
        "The `param_grid` in this example has `3 * 3 * 3 = 27` combinations and the cross validation folds are set `cv = 5` thus the time consuming training will be executed 135 times! \n",
        "\n",
        "The `verbose` parameter can be set (e.g. 1000) to get more info during execution. Using that it is easy to observe, that more complex models require more execution time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhTCIG1sun9z"
      },
      "source": [
        "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, return_train_score=True,\n",
        "                           scoring='neg_mean_squared_error', verbose=0)\n",
        "\n",
        "grid_search.fit(housing_prep, housing_labels);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvhd3KyhEpng"
      },
      "source": [
        "### Evaluate `GridSearchCV` results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzi8N85gun90"
      },
      "source": [
        "The best hyperparameter combination found:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25v42X4Kun91"
      },
      "source": [
        "grid_search.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_54StUaun95"
      },
      "source": [
        "Details on the *cross validation* results are also available as `dict`. Converting to a `DataFrame` makes it more readable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60m3kn1Qun96"
      },
      "source": [
        "pd.DataFrame(grid_search.cv_results_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VK5zznwG5-t"
      },
      "source": [
        "#### Exercise  \n",
        "Even as a `DataFrame` this is hard to evaluate.\n",
        "\n",
        "**Task**: Write a `get_best_of_cv_results` function, that:\n",
        "- takes the `dict` `grid_search.cv_results_` as input\n",
        "- converts it to a `pandas.DataFrame`\n",
        "- removes all unnecessary columns. Only keep those starting with `param_`\n",
        "- creates a `RSME` column: `RSME = squareroot(- mean_test_score)`\n",
        "- sorts by `RSME` with having the best on top\n",
        "- returns that beautified `DataFrame`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs7uvsgT3rF5"
      },
      "source": [
        "##### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eiozp3GwAsZ"
      },
      "source": [
        "def get_best_of_cv_results(results_dict):\n",
        "    df_all = pd.DataFrame(results_dict)\n",
        "\n",
        "    # select the columns to be kept\n",
        "    cols = [\"mean_test_score\"]\n",
        "    for col in df_all.columns:\n",
        "        if col.startswith(\"param_\"):\n",
        "            cols.append(col)\n",
        "    df = df_all.loc[:, cols]\n",
        "\n",
        "    # remove \"param_\" in column names\n",
        "    df.columns = [col.replace(\"param_\", \"\") for col in df.columns]\n",
        "\n",
        "    # convert negative mean squared error to RMSE\n",
        "    df[\"RMSE\"] = np.sqrt(-df[\"mean_test_score\"])\n",
        "    df.drop(labels=\"mean_test_score\", axis=1, inplace=True)\n",
        "\n",
        "    # sort by RMSE\n",
        "    df.sort_values(by=\"RMSE\", inplace=True)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJGAyTB13yMQ"
      },
      "source": [
        "##### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbhYBm79xtf6"
      },
      "source": [
        "get_best_of_cv_results(grid_search.cv_results_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO1PdG-LUfTe"
      },
      "source": [
        "Another helper function is created to visualize the results as heatmap:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whfdH1LyI2JP"
      },
      "source": [
        "def plot_pivot_heatmap(df, columns, index):\n",
        "    pt = df.pivot_table(columns=columns, index=index, values=\"RMSE\", aggfunc=np.min)\n",
        "    plt.figure(figsize=(4, 2.5))\n",
        "    plt.imshow(pt.values, interpolation='nearest', aspect='auto', cmap=plt.cm.coolwarm)\n",
        "    plt.yticks(range(len(pt.index)), pt.index)\n",
        "    plt.xticks(range(len(pt.columns)), pt.columns)\n",
        "    plt.xlabel(columns), plt.ylabel(index)\n",
        "    plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_LzvnNNJbFD"
      },
      "source": [
        "plot_pivot_heatmap(df=get_best_of_cv_results(grid_search.cv_results_), \n",
        "                   columns=\"max_features\", index=\"n_estimators\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntx4PkpRfw_R"
      },
      "source": [
        "Obervation: The more complex the model gets, the better it predicts.\n",
        "\n",
        "Interestingly, simple models (with a few estimators) perform better with higher min samples per leaf ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIfjE5w_JwHE"
      },
      "source": [
        "plot_pivot_heatmap(df=get_best_of_cv_results(grid_search.cv_results_), \n",
        "                   columns=\"min_samples_leaf\", index=\"n_estimators\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGf6_27lFSEJ"
      },
      "source": [
        "### Best estimator\n",
        "\n",
        "The **best estimator** found during `GridSearchCV` is dirctly available ...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wB3-4FIun92"
      },
      "source": [
        "grid_search.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiBPJAk8E8wv"
      },
      "source": [
        "## RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ9jGgYENRQz"
      },
      "source": [
        "`RandomizedSearchCV` implements a random alterative to the `GridSearchCV`. It is advantageous in case of many parameters and combinations and easier to scale to the available compute resources. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsR_13hqKUGu"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-92m8Hk7L3zR"
      },
      "source": [
        "In contrast to the `param_grid` of `GridSearchCV`, the `RandomizedSearchCV` expects a `param_distributions` `dict` with each `value` being a `list` or a distribution such as `scipy.stats.distributions`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa8pbP0ALulZ"
      },
      "source": [
        "from scipy.stats import randint\n",
        "\n",
        "param_distributions = {'n_estimators': randint(low=5, high=30),\n",
        "                       'max_features': randint(low=1, high=8)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsx1U52-un99"
      },
      "source": [
        "forest_reg = RandomForestRegressor(random_state=42)\n",
        "rnd_search = RandomizedSearchCV(forest_reg, param_distributions, n_iter=15, cv=5, \n",
        "                                scoring='neg_mean_squared_error', random_state=42,\n",
        "                                verbose=0)\n",
        "rnd_search.fit(housing_prep, housing_labels);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dofJC9CuVtcG"
      },
      "source": [
        "### Evaluate `RandomizedSearchCV` results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olDoDzC7vVl5"
      },
      "source": [
        "df = get_best_of_cv_results(rnd_search.cv_results_)\n",
        "df.plot.scatter(x=\"max_features\", y=\"n_estimators\", \n",
        "                c=\"RMSE\", s=100, cmap=mpl.cm.coolwarm,\n",
        "                figsize=(10,3), grid=True, sharex=False)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucra7Ih4gKmn"
      },
      "source": [
        "Again, more complex models predict better than simple ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "espx0_PLfb3r"
      },
      "source": [
        "## Feature importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myfHpkyIgT4r"
      },
      "source": [
        "The model we tuned with `GridSeachCV` and `RandomizedSearchCV` is the `RandomForestRegressor`. As a bycatch, that model indicates how important every feature is for making accurate predictions. Let's view them:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh_vIUkAun9_"
      },
      "source": [
        "grid_search.best_estimator_.feature_importances_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzEXdGzih2Ij"
      },
      "source": [
        "... for easy evaluation we need the column names along with the `feature_importances_` and also have them sorted: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZcSQBlwMVod"
      },
      "source": [
        "importances = pd.Series(grid_search.best_estimator_.feature_importances_, \n",
        "                        index=housing_prep_columns)\n",
        "importances.sort_values(inplace=True)\n",
        "importances.plot.barh(grid=True, title=\"feature importance\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUbnaFqOkrTy"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj9GS6rsjfMu"
      },
      "source": [
        "- `median_incame` stays the most important feature. This is somehow expected from the `corr_matrix` evaluation\n",
        "- all three added features are in the upper half\n",
        "- the `INLAND` 'bit' is 2nd most important. However, it seams all other categories could be dropped without losing much."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_dHAvvSn8nb"
      },
      "source": [
        "Let's plot again the linear correlations to the original features, for comparison:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5zXjv5uXJsS"
      },
      "source": [
        "corr = pd.DataFrame(np.column_stack([housing_prep, housing_labels]), \n",
        "                    columns=housing_prep_columns + [\"median_house_value\"]).corr()\n",
        "corr_value = corr[\"median_house_value\"].drop(\"median_house_value\")\n",
        "corr_value.abs().sort_values().plot.barh(grid=True, \n",
        "                                         title=\"linear correlation to median house value\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2C_VocWom-j"
      },
      "source": [
        "... interrestingly, the worst correlating feature `population_per_household` became the 3rd most important feature, at least for the `RandomForestRegressor` model. \n",
        "\n",
        "Complex non-linear models don't rely on simple linear relationship!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHhRDXfHFv0g"
      },
      "source": [
        "### Visualization of ``INLAND`` feature importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaMk9flGCZev"
      },
      "source": [
        "df = pd.DataFrame({\"median_house_value\": housing_labels,\n",
        "                   \"INLAND\": housing_prep[:, housing_prep_columns.index(\"INLAND\")]})\n",
        "for inland, label in zip([0, 1], [\"other\", \"INLAND\"]):\n",
        "    plt.hist(df.loc[df[\"INLAND\"]==inland, \"median_house_value\"], \n",
        "             bins=50, label=label)\n",
        "    plt.xlabel(\"median_house_value\"), plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIJcVDBQQeoA"
      },
      "source": [
        "for label in housing[\"ocean_proximity\"].unique():\n",
        "    df1 = housing[housing[\"ocean_proximity\"] == label]\n",
        "    plt.plot(df1[\"longitude\"], df1[\"latitude\"], \".\", label=label)\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lDDRcqhpjxg"
      },
      "source": [
        "### Potential future improvements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcjq1L5nt3t0"
      },
      "source": [
        "- dropping unimportant features\n",
        "- solve the clipping issue (esp. at 500.000)\n",
        "- clean the data further (e.g. remove outliers)\n",
        "- investigate the bad predictions and understand the root cause"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkl4YP4FtclL"
      },
      "source": [
        "# Final test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izzl6iDztidW"
      },
      "source": [
        "After tweaking your models for a while, you eventually decide that one of them performs *sufficiently* well. Then it's time to evaluate that final model with the test set. \n",
        "\n",
        "Once again, that must be the final step. Don't optimize any further on the test set!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv_bH9hqun-E"
      },
      "source": [
        "final_model = grid_search.best_estimator_\n",
        "\n",
        "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
        "y_test = strat_test_set[\"median_house_value\"].copy()\n",
        "\n",
        "X_test_prep = full_pipe.transform(X_test)   # not .fit_transform() !\n",
        "final_predictions = final_model.predict(X_test_prep)\n",
        "\n",
        "final_rmse = root_mean_squared_error(y_test, final_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLrXZiyOun-G"
      },
      "source": [
        "your model is ready for production!"
      ]
    }
  ]
}