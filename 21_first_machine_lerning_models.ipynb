{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "21_first_machine_lerning_models.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6ItOC_KAU0-w",
        "9EJk5z0pY0Xl",
        "8omFzhvafHwS",
        "1MhsxkXY9kqe",
        "moGKfevAVmKb",
        "9YT5uOMd4tVE",
        "EcBdEGIx4zc0"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMZWKbzHU6bbrIl3AtCXO95",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/munich-ml/MLPy2021/blob/main/21_first_machine_lerning_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNCA4Z_YVMg0"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4IYjY66VbWM"
      },
      "source": [
        "Some common imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOzZ9UW92ebn"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score, train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHW9W_USVVMT"
      },
      "source": [
        "Signal construction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmsyIG2p4DUt"
      },
      "source": [
        "def ideal_func(x):\n",
        "    return np.cos(np.pi * 1.7 * x)\n",
        "\n",
        "def real_func(x):\n",
        "    noise = 0.2 * np.random.randn(x.size).reshape(x.shape)\n",
        "    return ideal_func(x) + noise\n",
        "    \n",
        "np.random.seed(0)\n",
        "x_all = np.sort(np.random.rand(50)).reshape(50, 1)\n",
        "y_all = real_func(x_all)\n",
        "x_pred = np.linspace(0, 1, num=1000).reshape(1000, 1)\n",
        "x, x_test, y, y_test = train_test_split(x_all, y_all, test_size=0.5, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XloCtWp-lcb6"
      },
      "source": [
        "# Learning from data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaHilTOku5i7"
      },
      "source": [
        "From [wikipedia](https://en.wikipedia.org/wiki/Supervised_learning): \n",
        "\n",
        "**Supervised learning** is the machine learning task of learning a **function** that maps an input to an output based on example input-output pairs.\n",
        "\n",
        "Definitions in *machine learning* ...\n",
        "- function = **model**\n",
        "- learning = **training** the model \n",
        "- examples = **training data** (historical data)\n",
        "- example inputs = **training features**\n",
        "- example outputs = **training lables** (the target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maU7V54Tzbv_"
      },
      "source": [
        "So everything starts with the (training) data. In the 'Setup' section some features `x` and labels `y` are prepared:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10QGsveYMjB5"
      },
      "source": [
        "print(\"features x {}: {}\\nlabels y {}: {}\".format(x.shape, x[-3:].flatten(), y.shape, y[-3:].flatten()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8oPa-fWSW0r"
      },
      "source": [
        "pd.DataFrame(np.column_stack([x, y]), columns=[\"x\", \"y\"]).tail(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na-N4Vvf1Zl8"
      },
      "source": [
        "Note that the feature matrix has only one column. \n",
        "\n",
        "By that, the example stays as simple as possible. In real world *machine learning* projects there are hundreds or thousands of features (think of image classifiers). \n",
        "\n",
        "As an advantage 1D features can easily be flotted against their labels in a 2D plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt5gvnANMi6h"
      },
      "source": [
        "plt.scatter(x, y, label=\"training data\")\n",
        "plt.grid(), plt.legend(), plt.tight_layout();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpK_D-7Q2vsu"
      },
      "source": [
        "Now that we got the **data**, let's find a **model** that fits it (accuratly)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcQhqt-omGRC"
      },
      "source": [
        "# Linear regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUHhvv4q3KVj"
      },
      "source": [
        "The most simple model one can think of is a **linear regression model** with two fitting parameters ($a_0$ and $a_1$):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi3pn8P7ml7Q"
      },
      "source": [
        "$$y(x)=a_0 + a_1* x $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhcoBFFs3fbj"
      },
      "source": [
        "Note: For $N$ features (meaning `x` with $N$ columns) there would be $N$ pairs of fitting parameters ($a_0$ and $a_1$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62enMKJZ47P2"
      },
      "source": [
        "This **linear regression model**, as well as many other machine learning building block will be used from the **Scikit-learn** library\n",
        "\n",
        "https://scikit-learn.org/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d53xhiQ0NXa-"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPS2OJbn4eN0"
      },
      "source": [
        "The `model` is there, but it's untrained, yet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz3oPmCFQze3"
      },
      "source": [
        "## `model.fit()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5dSYgttNXYH"
      },
      "source": [
        "model.fit(x, y);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZedD-nfU6RkI"
      },
      "source": [
        "... fitting the model is a simple *one-liner*.\n",
        "\n",
        "Even better, **all Scikit-learn models** have the same basic interface with:\n",
        "- `.fit(x_train, y_train)` for fitting / training\n",
        "- `.predict(x_new)` predicting on new feature instances  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb10JY8tQ4CG"
      },
      "source": [
        "## `model.predict()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhccZwkmO8Z-"
      },
      "source": [
        "some_x = np.array([0.2, 0.5, 0.9]).reshape((-1, 1))\n",
        "some_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH2jMELpPCGW"
      },
      "source": [
        "some_y = model.predict(some_x)\n",
        "some_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-OU18jjPCDO"
      },
      "source": [
        "plt.scatter(x, y, label=\"training data\")\n",
        "plt.plot(some_x, some_y, \"-xm\", label=\"some predictions\")\n",
        "plt.grid(), plt.legend(), plt.tight_layout();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ItOC_KAU0-w"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwnMRPOFU7wC"
      },
      "source": [
        "- The **simple linear model** seems to be fitted correctly to the training data\n",
        "- However, prediction accuracy isn't great\n",
        "- The model **underfits** the data. For a more accurate fit the model isn't complex enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJHQHXl098ac"
      },
      "source": [
        "# Root mean squared error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haXRaZw5-TCV"
      },
      "source": [
        "Before improving the model further, we need a quantitative measure for the model's performance. There are two options:\n",
        "- a loss function: smaller is better\n",
        "- a utility function: bigger is better\n",
        "\n",
        "For the linear regression model, the per sample error\n",
        "$$ error = y_\\text{pred} - y_\\text{true}$$ can be plotted:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmO1LfEaWvNV"
      },
      "source": [
        "plt.scatter(x, y, label=\"training data\")\n",
        "plt.plot(x_pred, model.predict(x_pred), \"k\", label=\"linear regression model\")\n",
        "for xs, ys in zip(x, y):\n",
        "    yp = next(iter(model.predict([xs])))\n",
        "    plt.plot([xs, xs], [ys, yp], \"r\", label=\"error\" if xs == x[0] else \"\")\n",
        "plt.grid(), plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfUn0MOZzCHB"
      },
      "source": [
        "In the following, we will be using the *root mean squared error* or RMSE, defined as\n",
        "$$\\text{RMSE}(y_\\text{true}, y_\\text{pred}) = \\sqrt{\\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (y_\\text{pred} - y_\\text{true})^2}$$\n",
        "*Scikit-learn* provides `mean_squared_error` ready to use. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcY8CA5Q_K9C"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoWhxGmE_M2-"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "1. Write a wrapper function ``root_mean_squared_error(y_true, y_pred)`` that uses ``root_mean_squared_error`` and returns the ``RSME`` error\n",
        "\n",
        "1. Compute the ``RSME`` for the trained model\n",
        "\n",
        "1. Compute the ``RSME`` for a guess with all zeros\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZMWOmNTAJgf"
      },
      "source": [
        "### Solutions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgEmOlk99sak"
      },
      "source": [
        "def root_mean_squared_error(y_true, y_pred, verbose=1):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    if verbose > 0:\n",
        "        print(\"RMSE={:.2f}\".format(rmse))\n",
        "    return rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tABM9bw--2XZ"
      },
      "source": [
        "root_mean_squared_error(y, model.predict(x));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJGGGGvs_gE6"
      },
      "source": [
        "That is significant error!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN-euYwSZy0Y"
      },
      "source": [
        "root_mean_squared_error(y, np.zeros_like(y));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCHODi-JZJcK"
      },
      "source": [
        "root_mean_squared_error(y, np.zeros(y.shape));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLtxMOpz8heX"
      },
      "source": [
        "# Polynominal features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCiTqQ0-YJgo"
      },
      "source": [
        "We need a more powerful model. Let's stay with a linear regression model like\n",
        "$$y(x)=a_0 + a_1* x $$\n",
        "but add higher order features\n",
        "$$y(x)=a_0 + a_1* x+ a_2*x^2 + ... + a_N*x^N$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvyIASMNWvTw"
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8omFzhvafHwS"
      },
      "source": [
        "### Unterstanding **Polynominal features**\n",
        "\n",
        "Example with 2 input features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTmW3L9xa6yJ"
      },
      "source": [
        "df_2f = pd.DataFrame([[10, 0.9], [15, 0.7], [20, 0.8]], columns=[\"x0\", \"x1\"])\n",
        "df_2f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx9zpQEadYdC"
      },
      "source": [
        "pol = PolynomialFeatures(2)\n",
        "pol_vals = pol.fit_transform(df_2f.values)\n",
        "pd.DataFrame(pol_vals, columns=pol.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Phhh4ZlHpeSo"
      },
      "source": [
        "Example with 1 input feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g9Ib-OgpbuE"
      },
      "source": [
        "df_1f = df_2f[[\"x0\"]]\n",
        "pol = PolynomialFeatures(3)\n",
        "pol_vals = pol.fit_transform(df_1f.values)\n",
        "pd.DataFrame(pol_vals, columns=pol.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MhsxkXY9kqe"
      },
      "source": [
        "### Linear regression helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fLJ-Qwn6LL4"
      },
      "source": [
        "def plot_lin_reg_fit(plot_train_data=True, plot_test_data=False, plot_ideal=False,\n",
        "                     degrees=None):\n",
        "    \"\"\"\n",
        "    Plots linear regression models with various polynominal degrees.\n",
        "    \"\"\"\n",
        "    if degrees == None:\n",
        "        degrees = [1, 5, 15]\n",
        "\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    \n",
        "    if plot_ideal:\n",
        "        plt.plot(x_pred, ideal_func(x_pred), \"-.k\", label=\"ideal\")\n",
        "\n",
        "    if plot_train_data:\n",
        "        plt.scatter(x, y, label=\"training data\")\n",
        "\n",
        "    if plot_test_data:\n",
        "        plt.scatter(x_test, y_test, label=\"test data\")\n",
        "\n",
        "    for degree in degrees:\n",
        "        model = Pipeline([(\"poly\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
        "                            (\"lin_reg\", LinearRegression())])\n",
        "        model.fit(x, y)\n",
        "        y_pred = model.predict(x_pred)\n",
        "        plt.plot(x_pred, y_pred, label=\"model degree \"+str(degree))\n",
        "\n",
        "    plt.grid(), plt.legend(), plt.ylim((-1.5, 1.5))\n",
        "\n",
        "def plot_lin_rmse(plot_test_rmse=False, max_degree=15):\n",
        "    \"\"\"\n",
        "    Plots RMSE of linear regression models with various polynominal degrees.\n",
        "    \"\"\"\n",
        "    degrees = np.linspace(1, max_degree, max_degree, dtype=np.int8)\n",
        "    rmse_train, rmse_test = [], []\n",
        "    for degree in degrees:\n",
        "        model = Pipeline([(\"poly\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
        "                            (\"lin_reg\", LinearRegression())])\n",
        "        model.fit(x, y)\n",
        "        for rmse, xs, ys in zip([rmse_train, rmse_test],\n",
        "                            [x, x_test],\n",
        "                            [y, y_test]):\n",
        "            rmse.append(root_mean_squared_error(ys, model.predict(xs), verbose=0))\n",
        "\n",
        "    df = pd.DataFrame(np.column_stack([degrees, rmse_train, rmse_test]), columns=[\"model degree\", \"training\", \"test\"])\n",
        "    columns = [\"training\"]\n",
        "    if plot_test_rmse:\n",
        "        columns.append(\"test\")\n",
        "    df.plot.bar(x=\"model degree\", y=columns, xlim=(1, max_degree), ylim=(0, 0.8), \n",
        "            figsize=(8, 3), grid=True, title=\"root mean squared error\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moGKfevAVmKb"
      },
      "source": [
        "### Evaluating higher order models\n",
        "The `plot_lin_reg_fit` and `plot_lin_rmse` helper functions support easy evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUxayqEUPNRY"
      },
      "source": [
        "plot_lin_reg_fit(degrees=[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wuez88jcPNTr"
      },
      "source": [
        "plot_lin_reg_fit(degrees=[1, 3, 5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxnbnaYhBAVz"
      },
      "source": [
        "plot_lin_rmse()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRMjQ-cuC5uk"
      },
      "source": [
        "The RSME continous to decrease with higher model degree. \n",
        "\n",
        "But wait! Let's quickly reconfirm the \"high-order models\"..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woAb_2plPNV9"
      },
      "source": [
        "plot_lin_reg_fit(degrees=[1, 5, 10, 15])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXbdzG0Aw0g0"
      },
      "source": [
        "# Generalization problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF9-veWThWlq"
      },
      "source": [
        "The RSME of `model degree 10` is better than `model degree 5` and `model degree 15` is even better.\n",
        "\n",
        "However, judging the models qualitatively doesn't quite confirm the RSME numbers.\n",
        "\n",
        "Luckily, we didn't train the models on the entire data set. Instead, we've taken half of the data set apart as **test set**. That is fresh data, the model hasn't seen before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wfyecm5MgsVu"
      },
      "source": [
        "len(x), len(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQMiBv7nS8v0"
      },
      "source": [
        "plot_lin_reg_fit(plot_test_data=True, degrees=[1, 5, 15])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pezql3PikXbL"
      },
      "source": [
        "We can calculate the RMSE on the **test set** separately:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YtHWzMLRYda"
      },
      "source": [
        "plot_lin_rmse(plot_test_rmse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrQgEPafkk81"
      },
      "source": [
        "*Test RSME* being far worse than *training RSME* is a clear evidence of **overfitting**.\n",
        "\n",
        "- Overfitting models don't **generalize** well. Instead they **memorize** the training data. \n",
        "\n",
        "- The common root cause of overfitting is **too few training data** applied to a **too complex model**.\n",
        "\n",
        "- **Regularization** helps to avoids overfitting of complex models.\n",
        "\n",
        "The model is **underfitting** if both *RSME's* are bad. There are various potential root causes: \n",
        "- Too simple model,\n",
        "- labels unpredictable from features (e.g. probability of rain isn't accuratly predictable given the day of the week) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0eBYUIIrIoi"
      },
      "source": [
        "Plotting the **best fit model** along with `ideal` (the function that generated the data set) shows:\n",
        "- the **fit** is quite good\n",
        "- noise is not predictable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nafc5X3hUmo5"
      },
      "source": [
        "plot_lin_reg_fit(plot_test_data=True, plot_ideal=True, degrees=[5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnjb3Kr1l8Rw"
      },
      "source": [
        "# Decision tree model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcvnbDWx4ba8"
      },
      "source": [
        "Let's try a very different type of model, a **Decision tree model** on the same data.\n",
        "\n",
        "The interface is identical to the `LinearRegression` model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFeo5YZi3v1m"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "model = DecisionTreeRegressor()\n",
        "model.fit(x, y)\n",
        "\n",
        "some_x = np.array([0.2, 0.5, 0.9]).reshape((-1, 1))\n",
        "model.predict(some_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YT5uOMd4tVE"
      },
      "source": [
        "### Decision tree helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eJwtCf5eOxd"
      },
      "source": [
        "from sklearn import tree\n",
        "\n",
        "def plot_tree_fit(plot_train_data=True, plot_test_data=False, plot_tree=False, \n",
        "                  max_depth=2, min_samples_leaf=None):\n",
        "    \"\"\"\n",
        "    Plots the fitted function of various decision tree models.\n",
        "    \"\"\"\n",
        "    if min_samples_leaf == None:\n",
        "        min_samples_leaf = [2]\n",
        "\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    \n",
        "    if plot_train_data:\n",
        "        plt.scatter(x, y, label=\"training data\")\n",
        "\n",
        "    if plot_test_data:\n",
        "        plt.scatter(x_test, y_test, label=\"test data\")\n",
        "\n",
        "    for min_samples in min_samples_leaf:\n",
        "        model = DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=min_samples)\n",
        "        model.fit(x, y);\n",
        "        y_pred = model.predict(x_pred)\n",
        "        plt.plot(x_pred, y_pred, label=\"min samples per leaf = \"+str(min_samples))\n",
        "\n",
        "    plt.grid(), plt.legend(), plt.ylim((-1.5, 1.5))\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if plot_tree:\n",
        "        plt.figure(figsize=(8,7))\n",
        "        tree.plot_tree(model)\n",
        "\n",
        "\n",
        "def plot_tree_rmse(plot_test_rmse=True):\n",
        "    \"\"\"\n",
        "    Plots RMSE of various decision tree models.\n",
        "    \"\"\"\n",
        "    MAX_LEAF = 15\n",
        "    min_samples_leaf = np.linspace(MAX_LEAF, 1, MAX_LEAF, dtype=np.int8)\n",
        "    rmse_train, rmse_test = [], []\n",
        "    for min_samples in min_samples_leaf:\n",
        "        model = DecisionTreeRegressor(max_depth=100, min_samples_leaf=min_samples)\n",
        "        model.fit(x, y);\n",
        "        for rmse, xs, ys in zip([rmse_train, rmse_test],\n",
        "                            [x, x_test],\n",
        "                            [y, y_test]):\n",
        "            rmse.append(root_mean_squared_error(ys, model.predict(xs), verbose=0))\n",
        "\n",
        "    df = pd.DataFrame(np.column_stack([min_samples_leaf, rmse_train, rmse_test]), \n",
        "                      columns=[\"min samples per leaf\", \"training\", \"test\"])\n",
        "    columns = [\"training\"]\n",
        "    if plot_test_rmse:\n",
        "        columns.append(\"test\")\n",
        "    df.plot.bar(x=\"min samples per leaf\", y=columns, xlim=(1, MAX_LEAF), ylim=(0, 0.8), \n",
        "            figsize=(8, 3), grid=True, title=\"root mean squared error\");\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcBdEGIx4zc0"
      },
      "source": [
        "### Decision tree evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1C69jU_j3E_"
      },
      "source": [
        "plot_tree_fit(max_depth=2, min_samples_leaf=[2], plot_tree=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfRI9T6O6-WB"
      },
      "source": [
        "Both parameters, `max_depth` and `min_samples_leaf` can be used for **regularization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1zwJ2FKj4_P"
      },
      "source": [
        "plot_tree_fit(max_depth=10, min_samples_leaf=[1, 3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSGDLOljjGRI"
      },
      "source": [
        "plot_tree_rmse()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaNqkkiz8OhU"
      },
      "source": [
        "The \"1 sample per leaf\" model is clearly overfitting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2E9a3i90ETZ"
      },
      "source": [
        "plot_tree_fit(max_depth=10, min_samples_leaf=[1], plot_test_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uje4YzNm8l1t"
      },
      "source": [
        "\"3 samples per leaf\" seem to be the *best fit model*: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iteSn6c30EQv"
      },
      "source": [
        "plot_tree_fit(max_depth=10, min_samples_leaf=[3], plot_test_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5COMvOGt2Nfo"
      },
      "source": [
        "## Decision tree model summary\n",
        "- non-linear models\n",
        "- decision rules are derived from the training data\n",
        "- usable as 'white box models', because the rules allow easy interpretetation\n",
        "- more details at https://scikit-learn.org/stable/modules/tree.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQUrZgCB1nNa"
      },
      "source": [
        "# Scikit-Learn ressources\n",
        "Goto [Scikit-Learn website](https://scikit-learn.org/) for further reading or check-out the [Scikit_Learn_cheat_sheet](cheat_sheets/Scikit_Learn_cheat_sheet.pdf).\n",
        "\n",
        "[![images/sklearn_ml_map.png](https://github.com/munich-ml/MLPy2021/blob/main/images/sklearn_ml_map.png?raw=1)](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)"
      ]
    }
  ]
}